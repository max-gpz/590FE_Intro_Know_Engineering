---
output: pdf_document
---

# FE590.  Assignment #3 - Gang Ping Zhu


## Enter Your Name Here, or "Anonymous" if you want to remain anonymous..
## `r format(Sys.time(), "%Y-%m-%d")`


# Instructions

In this assignment, you should use R markdown to answer the questions below.  Simply type your R code into embedded chunks as shown above.

When you have completed the assignment, knit the document into a PDF file, and upload _both_ the .pdf and .Rmd files to Canvas.

Note that you must have LaTeX installed in order to knit the equations below.  If you do not have it installed, simply delete the questions below.

```{r}
library(ISLR)
library(boot)
library(leaps)
library(glmnet)
library(caTools)
library(pls)
library(tree)
library(randomForest)
library(gbm)
```

# Question 1 (based on JWHT Chapter 5, Problem 8)

In this problem, you will perform cross-validation on a simulated data set.

Generate a simulated data set as follows:
```{r}
set.seed(1)
y <- rnorm(100)
x <- rnorm(100)
y <- x - 2*x^2 + rnorm(100)
```

   (a) In this data set, what is _n_ and what is _p_?

```{r}
df <- cbind.data.frame(x, y)
```
   n = 100
   p = 1
   
   (b) Create a scatterplot of _x_ against _y_. Comment on what you find.
   
```{r}
plot(x,y)
```
   There is heavy curvature to the plot where we see an increase in Y when X gets closer to 0.
   
   (c) Set a random seed of 2, and then compute the LOOCV errors that result from fitting the following four models using least squares:
      1.  $Y = \beta_0 + \beta_1 X + \epsilon$
      2.  $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon$
      3.  $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon$
      4.  $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \beta_4 X^4 + \epsilon$
      
```{r}
set.seed(2)
glm.fit1 <- glm(y ~ x,data=df)
cv.err1 <- cv.glm(df, glm.fit1)
cv.err1$delta

glm.fit2 <- glm(y ~ x + poly(x,2),data=df)
cv.err2 <- cv.glm(df, glm.fit2)
cv.err2$delta

glm.fit3 <- glm(y ~ x + poly(x,2) + poly(x,3),data=df)
cv.err3 <- cv.glm(df, glm.fit3)
cv.err3$delta

glm.fit4 <- glm(y ~ x + poly(x,2) + poly(x,3) + poly(x,4),data=df)
cv.err4 <- cv.glm(df, glm.fit4)
cv.err4$delta

```
   (d) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.
   The second model had the smallest LOOCV error. This is as expected as it's the model that closely resemebles the y equation.

   (e) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawnbased on the cross-validation results?
   Yes, these results agree with the cross-validation results as our p-value indicates. Our second model, which mostly resembled y, showed the lowest test error.


# Question 2 (based on JWHT Chapter 6, Problem 8)

In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.

   (a) Set the random seed to be 10.  Use the `rnorm()` function to generate a predictor `X` of length `n = 100,` as well as a noise vector $\epsilon$ of length `n = 100.`
   
```{r}
set.seed(10)
x <- rnorm(100)
epsil <- rnorm(100)

```
   (b) Generate a response vector `Y` of length `n = 100` according to the model $$Y = 4 + 3 X + 2 X^2 + X^3 + \epsilon.$$

```{r}
y <- 4 + 3*x + 2*x^2 + x^3 + epsil
```
(c) Use the `regsubsets()` function to perform best subset selection in order to choose the best model containing the predictors $X, X^2, \ldots, X^10.$ What is the best model obtained according to $C_p,$ BIC, and adjusted $R^2$? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both X and Y .

```{r}
df2 <- data.frame(x, y)
r.fit <- regsubsets(y ~ poly(x, 10), df2, nvmax = 10)
r.sum <- summary(r.fit)
names(r.sum)
r.sum$rsq
```
According to the $R^2$ from the summary, the last model (the one with 10 variables) is the best model with a value of 0.9664680.  

```{r}
r.sum$cp
```
According to the $C_p,$ from the summary, the third model (containing three variables) is the best one with a value of 1.4155758.

```{r}
r.sum$bic
```
According to the BIC from the summary, the third model (containing three variables) is the best one with a value of -316.2625.

```{r}
plot(r.sum$rsq, type = "l")
plot(r.sum$cp, type = "l")
plot(r.sum$bic, type = "l")
plot(r.fit, scale = "r2")
plot(r.fit, scale = "Cp")
plot(r.fit, scale = "bic")
```

   (d) Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?

```{r}
r.fitfwd <- regsubsets(y ~ poly(x, 10), df2, nvmax = 10, method = "forward")
r.sumfwd <- summary(r.fitfwd)
r.sumfwd$rsq
r.sumfwd$cp
r.sumfwd$bic
plot(r.sumfwd$rsq, type = "l")
plot(r.sumfwd$cp, type = "l")
plot(r.sumfwd$bic, type = "l")
plot(r.fitfwd, scale = "r2")
plot(r.fitfwd, scale = "Cp")
plot(r.fitfwd, scale = "bic")

```

The forward step selection returned similar results where the model with 3 variables is the best for CP and BIC. For the $R^2$, the model with 10 variables is the best. 

```{r}
r.fitbwd <- regsubsets(y ~ poly(x, 10), df2, nvmax = 10, method = "backward")
r.sumbwd <- summary(r.fitbwd)
r.sumbwd$rsq
r.sumbwd$cp
r.sumbwd$bic
plot(r.sumbwd$rsq, type = "l")
plot(r.sumbwd$cp, type = "l")
plot(r.sumbwd$bic, type = "l")
plot(r.fitbwd, scale = "r2")
plot(r.fitbwd, scale = "Cp")
plot(r.fitbwd, scale = "bic")

```

The backward step selection returned similar results to the forward step selection where the model with 3 variables is the best for CP and BIC. For the $R^2$, the model with 10 variables is the best. 

# Question 3 (based on JWHT Chapter 7, Problem 6)

In this exercise, you will further analyze the `Wage` data set.

(a) Perform polynomial regression to predict `wage` using `age.` Use cross-validation to select the optimal degree d for the polynomial. What degree was chosen? Make a plot of the resulting polynomial fit to the data.

```{r}
data(Wage)
set.seed(999)
all.errs <- rep(NA, 9)
for(i in 1:9){
  glm.fit <- glm(wage~poly(age, i), data=Wage)
  all.errs[i] <- cv.glm(Wage, glm.fit, K=9)$delta[2]
}
```

```{r}
plot(1:9,all.errs, xlab="Degree", ylab="CV error", type="b", pch=20, ylim=c(1590, 1700))
sd.p <- sd(all.errs)
min.p <- min(all.errs)
abline(h=min.p + 0.15 * sd.p, col="blue", lty=1)
abline(h=min.p - 0.15 * sd.p, col="blue", lty=1)
legend("topright", "0.15-standard deviation lines", col="blue", lty=1)

```
From the plot above, the degree of 3 is the smallest degree with a small CV error.

```{r}
f1 <- lm(wage~age, data=Wage)
f2 <- lm(wage~poly(age, 2), data=Wage)
f3 <- lm(wage~poly(age, 3), data=Wage)
f4 <- lm(wage~poly(age, 4), data=Wage)
f5 <- lm(wage~poly(age, 5), data=Wage)
f6 <- lm(wage~poly(age, 6), data=Wage)
f7 <- lm(wage~poly(age, 7), data=Wage)
f8 <- lm(wage~poly(age, 8), data=Wage)
f9 <- lm(wage~poly(age, 9), data=Wage)
f10 <- lm(wage~poly(age, 10), data=Wage)
anova(f1, f2, f3, f4, f5, f6, f7, f8, f9, f10)

```
The anova shows that all the polynomials above 3 are not significant

```{r}
plot(wage~age, data=Wage)

lmfit <- lm(wage~poly(age, 3), data=Wage)

agelms <- range(Wage$age)
agegrid <- seq(from=agelms[1], to=agelms[2])

lmpred <- predict(lmfit, data.frame(age=agegrid))

lines(agegrid, lmpred, col="red", lwd=3)

```

(b) Fit a step function to predict `wage` using `age`, and perform cross-validation to choose the optimal number of cuts. Make a plot of the fit obtained.

```{r}
cvserrs <- rep(NA, 10)
dfage <- Wage$age

for (i in 2:10) {
  Wage$age.cut <- cut(dfage, i)
  lm.fit <- glm(wage~age.cut, data=Wage)
  cvserrs[i] <- cv.glm(Wage, lm.fit, K=10)$delta[2]
}
```

```{r}
plot(2:10, cvserrs[-1], xlab="Number of cuts", ylab="CV error", type="b", pch=20)
```

Based on the chart above, we'll use 8 cuts as our optimal number to plot.
```{r}
plot(wage~age, data=Wage)

lm.fit <- glm(wage~cut(age, 8), data=Wage)

agelms <- range(dfage)
agegrid <- seq(from=agelms[1], to=agelms[2])

lmpred <- predict(lm.fit, data.frame(age=agegrid))

lines(agegrid, lmpred, col="red", lwd=3)

```


# Question 4 (based on JWHT Chapter 8, Problem 8)

In the lab, a classification tree was applied to the `Carseats` data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.

(a) Split the data set into a training set and a test set.
```{r}
dfcs <- Carseats
dim(dfcs)
set.seed(55)
n <- 1:floor(nrow(dfcs)/2)
TrainingSet <- dfcs[n, ]
TestSet <- dfcs[ - n, ]
dim(TrainingSet)
dim(TestSet)
```

(b) Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?

```{r}
treecs <- tree(Sales ~ ., data = TrainingSet)
summary(treecs)
plot(treecs)
text(treecs)
```
```{r}
mean((TestSet$Sales - predict(treecs, TestSet))^2)
```
(c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?
```{r}
set.seed(77)
cv.cs <- cv.tree(treecs, FUN = prune.tree)

plot(cv.cs$size, cv.cs$dev, type="b")
```

```{r}
prunedcs <- prune.tree(treecs, best = 8)
plot(prunedcs)
text(prunedcs)
```

```{r}
mean((TestSet$Sales - predict(prunedcs, TestSet))^2)
```
Pruning improved it to 4.92.

(d) Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the `importance()` function to determine which variables are most important.

```{r}
set.seed(111)
bcs= randomForest(Sales ~ ., data = TrainingSet, ntree = 1000, mtry = 5, importance = TRUE)
mean((TestSet$Sales - predict(bcs, TestSet))^2)
importance(bcs)
```
The MSE is 2.57 and the four important predictors for Sale are CompPrice, Advertising, Price, and Shelve Location. 

# Question 5 (based on JWTH Chapter 8, Problem 10)

Use boosting (and bagging) to predict Salary in the Hitters data set

(a) Remove the observations for which salary is unknown, and then log-transform the salaries
```{r}
dfh <- Hitters
dfh <- na.omit(dfh)
dfh$LogSalary <- log(dfh$Salary)
```
(b) Split the data into training and testing sets for cross validation purposes.
```{r}
set.seed(50)
n <- 1:floor(nrow(dfh)/2)
HitTrainingSet <- dfh[n, ]
HitTestSet <- dfh[ - n, ]
dim(HitTrainingSet)
dim(HitTestSet)
```

(c) Perform boosting on the training set with 1000 trees for a range of values of the shrinkage parameter $\lambda$.  Produce a plot with different shrinkage parameters on the x-axis and the corresponding training set MSE on the y-axis

```{r}
set.seed(66)
lambda <- 10^(seq(-10, -0.2, by = 0.1))
length(lambda)
```

```{r}
trerr <- rep(NA, 99)
set.seed(66)
for (i in 1:99) {
    boost.hitters <- gbm(LogSalary ~ ., data = HitTrainingSet, distribution = "gaussian", n.trees = 1000, shrinkage = lambda[i])
    pred.train <- predict(boost.hitters, HitTrainingSet, n.trees = 1000)
    trerr[i] <- mean((pred.train - HitTrainingSet$LogSalary)^2)
}
```

```{r}
plot(lambda, trerr, type = "p", xlab = "Shrinkage values", ylab = "Training MSE")
```
(d) Produce a plot similar to the last one, but this time using the test set MSE

```{r}
testerr <- rep(NA, 99)
set.seed(22)
for (i in 1:99) {
    boost.hitters <- gbm(LogSalary ~ ., data = HitTrainingSet, distribution = "gaussian", n.trees = 1000, shrinkage = lambda[i])
    pred.test <- predict(boost.hitters, HitTestSet, n.trees = 1000)
    testerr[i] <- mean((pred.test - HitTestSet$LogSalary)^2)
}
```

```{r}
plot(lambda, testerr, type ="p", xlab = "Shrinkage values", ylab = "Test MSE")
```

(e) Fit the model using two other regression techniques (from previous classes) and compare the MSE of those techniques to the results of these boosted trees.
```{r}
fitlm <- lm(LogSalary ~ ., data = HitTrainingSet)
mean((HitTestSet$LogSalary - predict(fitlm, HitTestSet))^2)
```

```{r}
x.train <- model.matrix(LogSalary ~ ., data = HitTrainingSet)

x.test <- model.matrix(LogSalary ~ ., data = HitTestSet)

y <- HitTrainingSet$LogSalary

lassofit <- glmnet(x.train, y, alpha = 1)

mean((HitTestSet$LogSalary - predict(lassofit, s = 0.01, newx = x.test))^2)
```

(f) Reproduce (c) and (d), but this time use bagging instead of boosting and compare to the boosted MSE's and the MSE's from (e)

```{r}
set.seed(333)
lambda <- 10^(seq(-10, -0.2, by = 0.1))
length(lambda)
```

```{r}
trainerror <- rep(NA, 99)
testerror <- rep(NA, 99)

for (i in 1:99) {
    baghit <- randomForest(LogSalary ~ ., data = HitTrainingSet, ntree = 1000, mtry = 20)
    trainerror[i] <- mean((HitTrainingSet$LogSalary - predict(baghit, HitTrainingSet, n.trees = 1000))^2)
    testerror[i] <- mean((HitTestSet$LogSalary - predict(baghit, HitTestSet, n.trees = 1000))^2)
}

```

```{r}
plot(lambda, trainerror, type = "b", xlab = "Shrinkage", ylab = "Train MSE")

plot(lambda, testerror, type = "b", xlab = "Shrinkage", ylab = "Test MSE")
```

```{r}
mean((HitTestSet$LogSalary - predict(baghit, newdata = HitTestSet))^2)
mean((HitTrainingSet$LogSalary - predict(baghit, newdata = HitTrainingSet))^2)

```

The test MSE for bagging is lower than the MSE for boosting.